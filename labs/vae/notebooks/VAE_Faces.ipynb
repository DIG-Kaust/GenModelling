{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example with Faces"
      ],
      "metadata": {
        "id": "00KiavEM1l3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing the necessary libraries"
      ],
      "metadata": {
        "id": "gWHPDx801gdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zhAhxVVGJyEP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "import skimage.io\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading data from http://vis-www.cs.umass.edu/lfw/<br>Labeled Faces in the Wild (LFW) dataset"
      ],
      "metadata": {
        "id": "1f9Lphsf1rQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nclDyb9ACw5O"
      },
      "outputs": [],
      "source": [
        "def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n",
        "                      images_name = \"lfw-deepfunneled\",\n",
        "                      dx=80,dy=80,\n",
        "                      dimx=64,dimy=64\n",
        "    ):\n",
        "\n",
        "    # download if not exists\n",
        "    if not os.path.exists(images_name):\n",
        "        print(\"images not found, donwloading...\")\n",
        "        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n",
        "        print(\"extracting...\")\n",
        "        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n",
        "        print(\"done\")\n",
        "        assert os.path.exists(images_name)\n",
        "\n",
        "    if not os.path.exists(attrs_name):\n",
        "        print(\"attributes not found, downloading...\")\n",
        "        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n",
        "        print(\"done\")\n",
        "\n",
        "    # read attrs\n",
        "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,)\n",
        "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
        "\n",
        "\n",
        "    # read photos\n",
        "    photo_ids = []\n",
        "    for dirpath, dirnames, filenames in os.walk(images_name):\n",
        "        for fname in filenames:\n",
        "            if fname.endswith(\".jpg\"):\n",
        "                fpath = os.path.join(dirpath,fname)\n",
        "                photo_id = fname[:-4].replace('_',' ').split()\n",
        "                person_id = ' '.join(photo_id[:-1])\n",
        "                photo_number = int(photo_id[-1])\n",
        "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
        "\n",
        "    photo_ids = pd.DataFrame(photo_ids)\n",
        "    # print(photo_ids)\n",
        "    # mass-merge\n",
        "    # (photos now have same order as attributes)\n",
        "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
        "\n",
        "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
        "\n",
        "    # print(df.shape)\n",
        "    # image preprocessing\n",
        "    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n",
        "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
        "                                .apply(lambda img: resize(img,[dimx,dimy]))\n",
        "\n",
        "    all_photos = np.stack(all_photos.values) #.astype('uint8')\n",
        "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
        "\n",
        "    return all_photos,all_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9waIon7C3ne",
        "outputId": "5b3efebd-aef8-4d5c-ab5d-6c1b54e102f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images not found, donwloading...\n",
            "extracting...\n",
            "done\n",
            "attributes not found, downloading...\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "all_photos, all_attrs = fetch_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VAE Model"
      ],
      "metadata": {
        "id": "XdPd12KX2C-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iWhj4_5GwwXc"
      },
      "outputs": [],
      "source": [
        "features = 16\n",
        "# defining a simple linear VAE\n",
        "class LinearVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearVAE, self).__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_features=12288, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=features*2)\n",
        "\n",
        "        )\n",
        "\n",
        "        # decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=features, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=12288)\n",
        "        )\n",
        "\n",
        "    # Transformation that allows us to use an arbitrary normal random variable insted of using a random variable with a standard normal distribution (mean = 0, variance = 1)\n",
        "    # This trick allows us to generate a latent vector from an arbitrary normal distribution\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "        return sample\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding\n",
        "        x = self.flatten(x).float()\n",
        "        x = self.encoder(x).view(-1, 2, features)\n",
        "        # getting `mu` and `log_var`\n",
        "        mu = x[:, 0, :] # the first feature values as mean\n",
        "        log_var = x[:, 1, :] # the other feature values as variance\n",
        "        # getting the latent vector through reparameterization\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "\n",
        "        # decoding\n",
        "        x = self.decoder(z)\n",
        "        reconstruction = torch.sigmoid(x)\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "    def sample(self, z):\n",
        "        generated = self.decoder(z)\n",
        "        generated = torch.sigmoid(generated)\n",
        "        generated = generated.view(-1, 64, 64, 3)\n",
        "        return generated\n",
        "\n",
        "    def get_latent_vector(self, x):\n",
        "        x = self.flatten(x).float()\n",
        "        x = self.encoder(x).view(-1, 2, features)\n",
        "        # getting `mu` and `log_var`\n",
        "        mu = x[:, 0, :] # the first feature values as mean\n",
        "        log_var = x[:, 1, :] # the other feature values as variance\n",
        "        # getting the latent vector through reparameterization\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAB77d-PYY76"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ac5ey7uIYY77"
      },
      "outputs": [],
      "source": [
        "def KL_divergence(mu, logsigma):\n",
        "    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n",
        "    return loss\n",
        "\n",
        "def log_likelihood(x, reconstruction):\n",
        "    loss = nn.BCELoss(reduction='sum')\n",
        "    return loss(reconstruction, x)\n",
        "\n",
        "def loss_vae(x, mu, logsigma, reconstruction):\n",
        "    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPJQu70eYY79"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dtCjfqXdYY79"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "criterion = loss_vae\n",
        "\n",
        "autoencoder = LinearVAE().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters())\n",
        "\n",
        "train_photos, val_photos, train_attrs, val_attrs = train_test_split(all_photos, all_attrs, train_size=0.9, shuffle=False)\n",
        "train_loader = torch.utils.data.DataLoader(train_photos, batch_size=32)\n",
        "val_loader = torch.utils.data.DataLoader(val_photos, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "60ad192b33c648f2aacd1efe490f6c64",
            "5112bafabaeb44c29fb60c90da52fb64",
            "dfed5c7925b64412b1a69550f1654c8e",
            "839aec80ef1545efa47c343fe3299bb8",
            "ea3e37eb68d04922862c353247788ab1",
            "65a5b61cc9fd4e73a6c9365ff3caf120",
            "e395f24558b44fbbbdbcd4cd2873ebb5",
            "29fce5d706394e969e1947f2ef0d34ca",
            "fe3493de64e7470a9e058857747a15f4",
            "f7522c4a89eb40ccacd0a07923a638b4",
            "6f759776022242d9831efecc3451a9a5"
          ]
        },
        "id": "pqtmSI3QXWZl",
        "outputId": "c87d1935-77ce-4d5d-bd1c-85b1e02c0358"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60ad192b33c648f2aacd1efe490f6c64"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    autoencoder.train()\n",
        "    train_losses_per_epoch = []\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        reconstruction, mu, logsigma = autoencoder(batch.to(device))\n",
        "        reconstruction = reconstruction.view(-1, 64, 64, 3)\n",
        "        loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses_per_epoch.append(loss.item())\n",
        "\n",
        "    train_losses.append(np.mean(train_losses_per_epoch))\n",
        "\n",
        "    autoencoder.eval()\n",
        "    val_losses_per_epoch = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "          reconstruction, mu, logsigma = autoencoder(batch.to(device))\n",
        "          reconstruction = reconstruction.view(-1, 64, 64, 3)\n",
        "          loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n",
        "          val_losses_per_epoch.append(loss.item())\n",
        "\n",
        "    val_losses.append(np.mean(val_losses_per_epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxW_8fkYY8B",
        "scrolled": true
      },
      "source": [
        "#### Let's see how VAE is working and reconstructing images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jd3BWM_YY8C"
      },
      "outputs": [],
      "source": [
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "      reconstruction, mu, logsigma = autoencoder(batch.to(device))\n",
        "      reconstruction = reconstruction.view(-1, 64, 64, 3)\n",
        "      result = reconstruction.cpu().detach().numpy()\n",
        "      ground_truth = batch.numpy()\n",
        "      break\n",
        "\n",
        "plt.figure(figsize=(8, 20))\n",
        "for i, (gt, res) in enumerate(zip(ground_truth[:5], result[:5])):\n",
        "  plt.subplot(5, 2, 2*i+1)\n",
        "  plt.imshow(gt)\n",
        "  plt.subplot(5, 2, 2*i+2)\n",
        "  plt.imshow(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-KV8E8YY8F"
      },
      "source": [
        "#### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOhhH-osYY8G"
      },
      "outputs": [],
      "source": [
        "z = np.array([np.random.normal(0, 1, 16) for i in range(10)])\n",
        "output = autoencoder.sample(torch.FloatTensor(z).to(device))\n",
        "\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i in range(output.shape[0]):\n",
        "  plt.subplot(output.shape[0] // 2, 2, i + 1)\n",
        "  generated = output[i].cpu().detach().numpy()\n",
        "  plt.imshow(generated)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vy3GZjiLQQQ"
      },
      "source": [
        "### Some more examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvT104NkMcVq"
      },
      "source": [
        "#### Let's generate latent vectors for two images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuUDzK08ILrX"
      },
      "outputs": [],
      "source": [
        "gt_0 = torch.FloatTensor([ground_truth[0]]).to(device)\n",
        "gt_1 = torch.FloatTensor([ground_truth[1]]).to(device)\n",
        "\n",
        "first_latent_vector = autoencoder.get_latent_vector(gt_0)\n",
        "second_latent_vector = autoencoder.get_latent_vector(gt_1)\n",
        "\n",
        "plt.imshow(autoencoder.sample(first_latent_vector)[0].cpu().detach().numpy())\n",
        "plt.imshow(autoencoder.sample(second_latent_vector)[0].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS4Ns1EKmDsH"
      },
      "source": [
        "#### Let's now see the result of combining the above images: $\\alpha l_1 + (1 - \\alpha)l_2$, where $l_1, l_2$ – latent vectors, $\\alpha \\in [0, 1]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVUJFTjxKrwE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 5))\n",
        "for i, alpha in enumerate(np.linspace(0., 1., 5)):\n",
        "  plt.subplot(1, 5, i + 1)\n",
        "  latent = (1 - alpha) * first_latent_vector + alpha * second_latent_vector\n",
        "  img = autoencoder.sample(latent)[0].cpu().detach().numpy()\n",
        "  plt.imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60ad192b33c648f2aacd1efe490f6c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5112bafabaeb44c29fb60c90da52fb64",
              "IPY_MODEL_dfed5c7925b64412b1a69550f1654c8e",
              "IPY_MODEL_839aec80ef1545efa47c343fe3299bb8"
            ],
            "layout": "IPY_MODEL_ea3e37eb68d04922862c353247788ab1"
          }
        },
        "5112bafabaeb44c29fb60c90da52fb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65a5b61cc9fd4e73a6c9365ff3caf120",
            "placeholder": "​",
            "style": "IPY_MODEL_e395f24558b44fbbbdbcd4cd2873ebb5",
            "value": "  6%"
          }
        },
        "dfed5c7925b64412b1a69550f1654c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29fce5d706394e969e1947f2ef0d34ca",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe3493de64e7470a9e058857747a15f4",
            "value": 3
          }
        },
        "839aec80ef1545efa47c343fe3299bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7522c4a89eb40ccacd0a07923a638b4",
            "placeholder": "​",
            "style": "IPY_MODEL_6f759776022242d9831efecc3451a9a5",
            "value": " 3/50 [01:51&lt;28:58, 37.00s/it]"
          }
        },
        "ea3e37eb68d04922862c353247788ab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a5b61cc9fd4e73a6c9365ff3caf120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e395f24558b44fbbbdbcd4cd2873ebb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29fce5d706394e969e1947f2ef0d34ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe3493de64e7470a9e058857747a15f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7522c4a89eb40ccacd0a07923a638b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f759776022242d9831efecc3451a9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}